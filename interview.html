<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Improvyu - Your Personal AI Interview</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Orbitron:wght@500;700&display=swap" rel="stylesheet">
    <!-- Three.js for 3D Avatar -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <!-- TensorFlow.js and Face-API.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@1.7.4"></script>
    <script src="https://unpkg.com/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #0c0a1a;
            color: #e0e0e0;
            overflow-x: hidden;
        }
        h1, h2, h3, .font-orbitron { font-family: 'Orbitron', sans-serif; }
        .glass-card {
            background: rgba(255, 255, 255, 0.05);
            backdrop-filter: blur(12px); -webkit-backdrop-filter: blur(12px);
            border: 1px solid rgba(255, 255, 255, 0.1); border-radius: 1rem;
        }
        .cta-button {
            background: linear-gradient(90deg, #4f46e5, #c026d3);
            transition: all 0.3s ease;
            box-shadow: 0 0 20px rgba(192, 38, 211, 0.6);
        }
        .cta-button:hover:not(:disabled) {
            transform: translateY(-2px) scale(1.03);
            box-shadow: 0 0 30px rgba(192, 38, 211, 0.9);
        }
        .cta-button:disabled {
            background: #333; box-shadow: none;
            cursor: not-allowed; opacity: 0.5;
        }
        #video-container {
            position: relative;
            background-color: #000;
            border-radius: 0.5rem;
            overflow: hidden; /* Ensures canvas fits within the border radius */
        }
        #video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            transform: scaleX(-1); /* Mirror effect */
        }
        #overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        .loader {
            border: 4px solid rgba(255, 255, 255, 0.2);
            border-left-color: #a78bfa; border-radius: 50%;
            width: 50px; height: 50px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin { to { transform: rotate(360deg); } }
        .status-dot {
            width: 12px; height: 12px; border-radius: 50%;
            transition: background-color 0.3s ease;
        }
        .status-listening { background-color: #f43f5e; box-shadow: 0 0 8px #f43f5e; }
        .status-processing { background-color: #facc15; box-shadow: 0 0 8px #facc15; }
        .status-speaking { background-color: #4ade80; box-shadow: 0 0 8px #4ade80; }
        .status-idle { background-color: #6b7280; }
        .bg-red-600 { background-color: #dc2626; }
    </style>
</head>
<body class="relative min-h-screen">
    <!-- Header -->
    <header class="relative z-20 p-4 bg-[#0c0a1a]/50 backdrop-blur-sm border-b border-white/10">
        <div class="container mx-auto flex justify-between items-center">
            <div class="flex items-center space-x-3">
                <svg class="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.663 17h4.673M12 3v1m6.364 1.636l-.707.707M21 12h-1M4 12H3m3.343-5.657l-.707-.707m2.828 9.9a5 5 0 117.072 0l-.548.547A3.374 3.374 0 0014 18.469V19a2 2 0 11-4 0v-.531c0-.895-.356-1.754-.988-2.386l-.548-.547z"></path></svg>
                <h1 class="text-2xl font-bold text-white font-orbitron">Improvyu</h1>
            </div>
            <a href="/" class="bg-white/10 hover:bg-white/20 text-white font-semibold py-2 px-4 rounded-lg transition">Back to Home</a>
        </div>
    </header>

    <!-- Main Content -->
    <main class="container mx-auto p-4 md:p-8">
        <div id="upload-section" class="text-center glass-card p-8 max-w-2xl mx-auto">
            <h2 class="text-3xl font-bold mb-4">Begin Your Analysis</h2>
            <p class="text-gray-400 mb-6">Upload your resume (PDF) to generate a personalized interview experience.</p>
            <input type="file" id="resume-file" class="hidden" accept=".pdf">
            <label for="resume-file" class="cta-button inline-block text-white font-bold py-3 px-8 rounded-full text-lg cursor-pointer">Select Resume</label>
            <p id="file-name" class="mt-4 text-gray-300"></p>
            <div id="upload-loader" class="hidden mx-auto mt-4"><div class="loader"></div><p class="mt-2">Analyzing resume and generating questions...</p></div>
        </div>

        <div id="interview-container" class="hidden grid grid-cols-1 lg:grid-cols-5 gap-8 mt-8">
            <!-- Left Column: Interview Flow -->
            <div class="lg:col-span-3 glass-card p-6 h-full flex flex-col">
                <div id="avatar-container" class="relative bg-black rounded-lg h-40 md:h-48 mb-4">
                    <canvas id="avatar-canvas"></canvas>
                </div>
                <div class="flex items-center space-x-3 mb-4 p-2 bg-black/20 rounded-lg">
                    <div id="status-dot" class="status-dot status-idle"></div>
                    <span id="status-text" class="text-gray-400">Awaiting interview start...</span>
                </div>
                <div class="bg-black/20 p-4 rounded-lg flex-grow min-h-[150px]"><h3 id="question" class="text-xl text-purple-300">Welcome! Your personalized questions are ready.</h3></div>
                <div class="mt-4 flex items-center space-x-4"><button id="start-interview-btn" class="cta-button font-bold py-2 px-6 rounded-full">Start Interview</button><button id="record-btn" class="cta-button font-bold py-2 px-6 rounded-full" disabled>Record Answer</button></div>
                <div id="responses" class="mt-6 space-y-2 overflow-y-auto max-h-60"><h3 class="font-bold text-lg">Conversation Log:</h3></div>
            </div>
            <!-- Right Column: Video & Report -->
            <div class="lg:col-span-2 glass-card p-6 flex flex-col">
                <h2 class="text-2xl font-bold mb-4">Candidate Analysis</h2>
                <div id="video-container" class="w-full aspect-video">
                    <video id="video" autoplay muted playsinline></video>
                </div>
                <div id="report-results" class="mt-6 hidden"><h3 class="font-bold text-lg mb-2">Final Report:</h3><div id="report-content" class="bg-black/20 p-4 rounded-lg text-sm"></div></div>
            </div>
        </div>
    </main>

    <script>
        // --- State Management & DOM Elements ---
        let initialQuestions = [], conversationHistory = [], allAnswers = [];
        let initialQuestionPointer = 0;
        let speechRecognition, isRecording = false, currentTranscript = "";
        let femaleVoice = null;

        const uploadSection = document.getElementById('upload-section'), interviewContainer = document.getElementById('interview-container');
        const resumeFileInput = document.getElementById('resume-file'), fileNameEl = document.getElementById('file-name'), uploadLoader = document.getElementById('upload-loader');
        const questionEl = document.getElementById('question'), startInterviewBtn = document.getElementById('start-interview-btn'), recordBtn = document.getElementById('record-btn');
        const responsesEl = document.getElementById('responses'), reportResultsEl = document.getElementById('report-results'), reportContentEl = document.getElementById('report-content');
        const statusDot = document.getElementById('status-dot'), statusText = document.getElementById('status-text');
        const avatarCanvas = document.getElementById('avatar-canvas'), avatarContainer = document.getElementById('avatar-container');
        const video = document.getElementById('video'), videoContainer = document.getElementById('video-container');

        // --- 3D Avatar Setup (Three.js) ---
        let scene, camera, renderer, avatar;
        function initAvatar() {
            scene = new THREE.Scene();
            const containerRect = avatarContainer.getBoundingClientRect();
            camera = new THREE.PerspectiveCamera(75, containerRect.width / containerRect.height, 0.1, 1000);
            renderer = new THREE.WebGLRenderer({ canvas: avatarCanvas, antialias: true, alpha: true });
            renderer.setSize(containerRect.width, containerRect.height);
            renderer.setPixelRatio(window.devicePixelRatio);
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
            scene.add(ambientLight);
            const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
            directionalLight.position.set(5, 5, 5);
            scene.add(directionalLight);
            const avatarGroup = new THREE.Group();
            const headGeometry = new THREE.SphereGeometry(0.5, 32, 32);
            const material = new THREE.MeshStandardMaterial({ color: 0xa78bfa, roughness: 0.5 });
            const head = new THREE.Mesh(headGeometry, material);
            const torsoGeometry = new THREE.CylinderGeometry(0.3, 0.6, 1.2, 32);
            const torso = new THREE.Mesh(torsoGeometry, material);
            torso.position.y = -0.9;
            avatarGroup.add(head);
            avatarGroup.add(torso);
            avatar = avatarGroup;
            scene.add(avatar);
            camera.position.z = 2;
            animateAvatar();
            window.addEventListener('resize', onWindowResize, false);
        }
        function animateAvatar() {
            requestAnimationFrame(animateAvatar);
            const time = Date.now() * 0.0005;
            if (avatar) avatar.position.y = Math.sin(time) * 0.03;
            renderer.render(scene, camera);
        }
        function onWindowResize() {
            const containerRect = avatarContainer.getBoundingClientRect();
            if (containerRect.width > 0 && containerRect.height > 0) {
                camera.aspect = containerRect.width / containerRect.height;
                camera.updateProjectionMatrix();
                renderer.setSize(containerRect.width, containerRect.height);
            }
        }
        
        // --- Face API & Video Setup ---
        async function startFaceApi() {
            try {
                await Promise.all([
                    faceapi.nets.ssdMobilenetv1.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models'),
                    faceapi.nets.faceExpressionNet.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models')
                ]);
                startVideoStream();
            } catch (err) { console.error('Error loading face-api models:', err); }
        }
        function startVideoStream() {
            navigator.mediaDevices.getUserMedia({ video: {} })
                .then(stream => { video.srcObject = stream; })
                .catch(err => console.error("Error accessing webcam:", err));
        }
        video.addEventListener('play', () => {
            const canvas = faceapi.createCanvasFromMedia(video);
            canvas.id = 'overlay';
            videoContainer.append(canvas);
            
            setInterval(async () => {
                try {
                    const displaySize = { width: video.clientWidth, height: video.clientHeight };
                    faceapi.matchDimensions(canvas, displaySize);
                    const detections = await faceapi.detectAllFaces(video, new faceapi.SsdMobilenetv1Options()).withFaceExpressions();
                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    const context = canvas.getContext('2d');
                    context.clearRect(0, 0, canvas.width, canvas.height);
                    context.save();
                    context.scale(-1, 1);
                    context.translate(-canvas.width, 0);
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                    context.restore();
                    resizedDetections.forEach(result => {
                        const { expressions, detection } = result;
                        const bestExpression = Object.keys(expressions).reduce((a, b) => expressions[a] > expressions[b] ? a : b);
                        const text = `${bestExpression} (${Math.round(expressions[bestExpression] * 100)}%)`;
                        new faceapi.draw.DrawTextField([text], detection.detection.box.bottomLeft).draw(canvas);
                    });
                } catch (err) {}
            }, 200);
        });

        // --- Status & Logging ---
        function updateStatus(status, text) {
            statusDot.className = `status-dot ${status}`;
            statusText.textContent = text;
        }
        function logToConversation(role, text) {
            const p = document.createElement('p');
            const color = role === 'You' ? 'text-purple-300' : 'text-cyan-300';
            p.innerHTML = `<strong class="${color}">${role}:</strong> ${text}`;
            responsesEl.appendChild(p);
            responsesEl.scrollTop = responsesEl.scrollHeight;
        }

        // --- Resume & Interview Logic ---
        resumeFileInput.addEventListener('change', async (event) => {
            const file = event.target.files[0];
            if (!file || file.type !== 'application/pdf') { 
                alert("Please select a PDF file."); 
                return; 
            }
            fileNameEl.textContent = `Selected: ${file.name}`;
            uploadLoader.classList.remove('hidden');
            const formData = new FormData();
            formData.append('resume', file);
            
            try {
                const response = await fetch('/api/analyze', {
                    method: 'POST',
                    body: formData
                });

                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(errorData.error || `Server responded with status ${response.status}`);
                }
                
                const data = await response.json();
                if (data.error) { throw new Error(data.error); }

                initialQuestions = data.questions;
                uploadSection.classList.add('hidden');
                interviewContainer.classList.remove('hidden');
                initAvatar();
                startFaceApi();
                setupSpeechSynthesis();
            } catch (error) {
                alert(`Failed to analyze resume: ${error.message}`);
                uploadLoader.classList.add('hidden');
                fileNameEl.textContent = "";
            }
        });

        function setupSpeechSynthesis() {
            const setVoice = () => {
                const voices = window.speechSynthesis.getVoices();
                femaleVoice = voices.find(voice => /female|woman|zira|susan/i.test(voice.name) && /en-US|en-GB/i.test(voice.lang));
                if (!femaleVoice) {
                    femaleVoice = voices.find(voice => /en-US|en-GB/i.test(voice.lang));
                }
            };
            setVoice();
            if (window.speechSynthesis.onvoiceschanged !== undefined) {
                window.speechSynthesis.onvoiceschanged = setVoice;
            }
        }

        function speak(text) {
            return new Promise((resolve) => {
                updateStatus('status-speaking', 'AI is speaking...');
                const utterance = new SpeechSynthesisUtterance(text);
                if (femaleVoice) {
                    utterance.voice = femaleVoice;
                }
                utterance.pitch = 1;
                utterance.rate = 1;
                utterance.onend = () => {
                    updateStatus('status-idle', 'Ready for your answer.');
                    resolve();
                };
                window.speechSynthesis.speak(utterance);
            });
        }

        async function askQuestion() {
            let questionText;
            
            // This logic now correctly handles the full conversational flow.
            if (conversationHistory.length === 0) {
                // Ask the first question from the initial list.
                questionText = initialQuestions[initialQuestionPointer];
                initialQuestionPointer++;
            } else {
                updateStatus('status-processing', 'AI is thinking...');
                try {
                    // Attempt to get a conversational follow-up question.
                    const response = await fetch('/api/follow-up', {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ history: conversationHistory })
                    });
                    if (!response.ok) {
                        const errorData = await response.json();
                        throw new Error(errorData.error || `Server responded with status ${response.status}`);
                    }
                    const followUpData = await response.json();
                    const followUpText = followUpData.question;

                    // If the AI provides a real follow-up, use it.
                    if (followUpText && followUpText !== "[NEXT_QUESTION]") {
                        questionText = followUpText;
                    } 
                    // Otherwise, if there are still main questions left, ask the next one.
                    else if (initialQuestionPointer < initialQuestions.length) {
                        questionText = initialQuestions[initialQuestionPointer];
                        initialQuestionPointer++;
                    } 
                    // If we're out of main questions, end the interview.
                    else {
                        await speak("That concludes the interview. Generating your report now.");
                        recordBtn.disabled = true;
                        await processInterview();
                        return;
                    }
                } catch(err) {
                    // If the follow-up API fails, gracefully move to the next main question.
                    console.error("Error getting follow-up question, moving to next main question:", err);
                    if (initialQuestionPointer < initialQuestions.length) {
                         questionText = initialQuestions[initialQuestionPointer];
                         initialQuestionPointer++;
                    } else {
                         await speak("It seems we've reached the end. Generating report.");
                         recordBtn.disabled = true;
                         await processInterview();
                         return;
                    }
                }
            }
            
            conversationHistory.push({ role: 'model', parts: [{ text: questionText }] });
            questionEl.textContent = questionText;
            logToConversation('LENORA', questionText);
            recordBtn.disabled = true;
            await speak(questionText);
            recordBtn.disabled = false;
        }

        function toggleRecording() {
            if (isRecording) {
                speechRecognition.stop();
            } else {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                if (!SpeechRecognition) { alert("Speech Recognition is not supported in your browser."); return; }
                
                speechRecognition = new SpeechRecognition();
                speechRecognition.lang = "en-US";
                speechRecognition.continuous = false;
                currentTranscript = "";

                speechRecognition.onstart = () => {
                    isRecording = true;
                    updateStatus('status-listening', 'Listening...');
                    recordBtn.textContent = "Stop Recording";
                    recordBtn.classList.add('bg-red-600');
                };
                speechRecognition.onresult = (event) => {
                    currentTranscript = Array.from(event.results).map(result => result[0].transcript).join('');
                };
                speechRecognition.onend = () => {
                    isRecording = false;
                    recordBtn.textContent = "Record Answer";
                    recordBtn.classList.remove('bg-red-600');
                    if(currentTranscript) {
                       allAnswers.push(currentTranscript);
                       conversationHistory.push({ role: 'user', parts: [{ text: currentTranscript }] });
                       logToConversation('You', currentTranscript);
                    }
                    setTimeout(askQuestion, 100);
                };
                speechRecognition.start();
            }
        }
        
        async function processInterview() {
            updateStatus('status-processing', 'Generating final report...');
            reportResultsEl.classList.remove('hidden');
            reportContentEl.innerHTML = '<div class="loader mx-auto"></div><p class="text-center mt-2">Evaluating...</p>';
            try {
                const response = await fetch('/api/evaluate', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ answers: allAnswers })
                });
                 if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(errorData.error || `Server responded with status ${response.status}`);
                }
                const report = await response.json();

                if (report.error) { throw new Error(report.error); }
                
                reportContentEl.innerHTML = `
                    <p class="mb-2"><strong>Overall Score:</strong> ${report.overallScore || 'Not available'}</p>
                    <p class="mb-2"><strong>Strengths:</strong> ${report.strengths || 'Not available'}</p>
                    <p class="mb-2"><strong>Areas for Improvement:</strong> ${report.weaknesses || 'Not available'}</p>
                    <p><strong>Suggestions:</strong> ${report.suggestion || 'Not available'}</p>
                `;
                updateStatus('status-idle', 'Report complete.');
            } catch (error) {
                reportContentEl.innerHTML = `<p class="text-red-400">Error: Could not retrieve report results. ${error.message}</p>`;
            }
        }

        startInterviewBtn.addEventListener('click', () => {
            startInterviewBtn.disabled = true;
            startInterviewBtn.style.opacity = '0.5';
            askQuestion();
        });
        recordBtn.addEventListener('click', toggleRecording);

    </script>
</body>
</html>